{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpkUlBXg_fis"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNDDpLvy_fit"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQFu11xP_fit"
      },
      "outputs": [],
      "source": [
        "label2Id = {\"HEB\": 0, \"ARB\": 1, \"FRA\":2, \"RUS\": 3, \"ART\": 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating artificial essays from Hebrew Wikipedia sentences"
      ],
      "metadata": {
        "id": "4tyB19O6_rTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX1XV0Vv_fit"
      },
      "outputs": [],
      "source": [
        "max_sentence_size = 230\n",
        "with open('data/heb_wiki_sentences.txt', 'r', encoding='utf8') as f:\n",
        "    counter = 0\n",
        "    essays = []\n",
        "    sentence = ''\n",
        "    for line in f:\n",
        "        tokens = line.split()\n",
        "        end_sentence_char = np.random.choice([',', '.'], p=[0.75, 0.25])\n",
        "        sentence += line.strip() + end_sentence_char + ' '\n",
        "        counter += len(tokens)\n",
        "\n",
        "        if counter >= max_sentence_size:\n",
        "            essays.append(sentence[:-2] + '.')\n",
        "            sentence = ''\n",
        "            counter = 0\n",
        "\n",
        "pd.DataFrame({'text': essays, 'label': 4}).sample(1000).to_csv('data/essays_artificial.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La7jx_ZY_fit"
      },
      "outputs": [],
      "source": [
        "def extract_data(files_path: str, is_native=False, join_sentences=True) -> pd.DataFrame:\n",
        "\n",
        "    '''\n",
        "    Extract text data and labels from number of text files.\n",
        "    Args:\n",
        "        files_path (str) - path for text files.\n",
        "        is_native (bool, optional) - indicator whether the text files were originated by a native speaker.\n",
        "        join_sentences (bool, optional) - indicator whether to join all the sentences in a file to one text or keep them separated.\n",
        "    Output:\n",
        "        corpus (Dataframe) - all the texts and theirs corresponding label.\n",
        "    '''\n",
        "    corpus = pd.DataFrame(columns=['text','label'])\n",
        "    file_names = glob.glob(files_path)\n",
        "\n",
        "    for file_name in file_names:\n",
        "        if is_native:\n",
        "            label_id = label2Id['HEB']\n",
        "        else:\n",
        "            label = os.path.basename(file_name).split('_')[0]\n",
        "            label_id = label2Id[label]\n",
        "        essay = pd.read_csv(file_name, sep=';;;', header=None, names=['text'])\n",
        "        if join_sentences:\n",
        "            essay = pd.DataFrame(data={'text': [essay['text'].str.cat(sep=' ')], 'label':[label_id]})\n",
        "        else:\n",
        "            essay['label'] = label_id\n",
        "        corpus = pd.concat([corpus,essay], ignore_index=True)\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWHl0yt9_fiu"
      },
      "outputs": [],
      "source": [
        "non_native_corpus = extract_data(files_path = './data/Hebrew Corpus/NonNatives/TXT/Original/*.txt')\n",
        "native_corpus = extract_data(files_path = './data/Hebrew Corpus/Natives/Raw/*/*.txt', is_native=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R9WV1pn_fiu"
      },
      "outputs": [],
      "source": [
        "pd.concat([native_corpus, non_native_corpus]).to_csv('./data/essays.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84GGguMV_fiu",
        "outputId": "66beaf76-3a0f-456f-99b0-f3e72a0c30ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "מתוקף תפקידה של האקדמיה ללשון לבחון שינויים המתרחשים בשפה העברית\n",
            "(לדוגמא – כניסת עידן האינטרנט) ולהחליט להוסיפם לאוצר המילים העברי.\n",
            "אזרחי מדינת ישראל ברובם משלמים אגרת חובה לטובת רגולציה זו, אשר אמונה\n",
            "על לשון תיקנית ברדיו ובטלויזיה. עברית תקינה היא אבן דרך מרכזית תוך\n",
            "התהוות מדינה יש לשמרה תוך רגולציה של רשות השידור כזרע של משרד התקשורת.\n",
            "תקינות השפה העברית גם בשנות האלפיים מהווה אור לגויים וסדר ארגוני\n",
            "במסרים אשר מעבירים כלי התקשורת המשדרים שירים. לעניות דעתי, מחובת רשות\n",
            "השידור לפקח על כלי התקשורת הישראליים ולמנוע שידור שירים ישראליים\n",
            "(עבריים) בהם קיימת הגייה לא תקנית. עוד בימי אבותינו, חז\"ל ועוד רבים\n",
            "אחרים הקפידו ונתנו דגש בעת פסיקותיהם על הגייה נכונה של מילים תנכיות\n",
            "מתוך כוונה שהמילים לא \"יתבוללו\" יחד עם אוצר המילים של הגויים. למיטב\n",
            "ידיעתי, כמעט בכל שפה מדוברת ברחבי העולם, קיימת שפה ספרותית ושפה עממית,\n",
            "הנבדלים זה מזו בצורת ההגייה ובאופן השימוש בהן בחיי היום יום.\n"
          ]
        }
      ],
      "source": [
        "print(textwrap.fill(native_corpus.loc[0,'text'], replace_whitespace=False, fix_sentence_endings=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}